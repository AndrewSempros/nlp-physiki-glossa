{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59459f61",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = AutoModel.from_pretrained('bert-base-uncased').to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde241e6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def embed_text(text: str) -> torch.Tensor:\n",
    "    toks = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        out = model(**toks)\n",
    "    return out.last_hidden_state.mean(dim=1).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15586d32",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_keywords(text: str) -> list[str]:\n",
    "    toks = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "    return [w for w in toks if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "originals = {\n",
    "    'Text 1': (\n",
    "        \"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in \"\n",
    "        \"our lives. Hope you too, to enjoy it as my deepest wishes. Thank your message to show our words to the \"\n",
    "        \"doctor, as his next contract checking, to all of us. I got this message to see the approved message. In fact, \"\n",
    "        \"I have received the message from the professor, to show me, this, a couple of days ago. I am very appreciated \"\n",
    "        \"the full support of the professor, for our Springer proceedings publication.\"\n",
    "    ),\n",
    "    'Text 2': (\n",
    "        \"During our final discuss, I told him about the new submission — the one we were waiting since last autumn, \"\n",
    "        \"but the updates was confusing as it not included the full feedback from reviewer or maybe editor? Anyway, I believe \"\n",
    "        \"the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation. \"\n",
    "        \"We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link came finally last week, I think. \"\n",
    "        \"Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sends again. \"\n",
    "        \"Because I didn’t see that part final yet, or maybe I missed, I apologize if so. Overall, let us make sure all are safe and \"\n",
    "        \"celebrate the outcome with strong coffee and future targets.\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = {\n",
    "    'Text 1': {\n",
    "        'T5-PAWS': (\n",
    "            \"Today is our dragon boat festival in our Chinese culture to celebrate it with all safe and great in our lives. \"\n",
    "            \"I hope you too will enjoy it as my deepest wishes. Thank your message to show our words to the doctor as his next contract check. \"\n",
    "            \"I received this message to see the approved message. In fact, a couple of days ago I received the message from the professor to show me. \"\n",
    "            \"I am very thankful for the professor's full support for our Springer proceedings publication.\"\n",
    "        ),\n",
    "        'Pegasus': (\n",
    "            \"The dragon boat festival is celebrated in our Chinese culture and we should all be happy. \"\n",
    "            \"I hope you enjoy it as much as I do. Thank you for your message, which will be shown to the doctor. \"\n",
    "            \"I saw the approved message when I received this message. The professor sent a message to me a few days ago. \"\n",
    "            \"The professor was very supportive of the Springer proceedings publication.\"\n",
    "        ),\n",
    "        'Humarin': (\n",
    "            \"Our Chinese culture features a dragon boat festival today, where we commemorate it with all the good we have come to know. \"\n",
    "            \"Until next time, may it be just as special to you as it is to me. As the next contract checking came and went, thank you \"\n",
    "            \"for sending your message to the doctor to display what we had told him. I received this message to view the authorized message. \"\n",
    "            \"The professor sent me a message to show me a couple of days ago, and I have been in touch with him ever since. \"\n",
    "            \"The prof's complete endorsement of our Springer proceedings publication receives my complete appreciation.\"\n",
    "        )\n",
    "    },\n",
    "    'Text 2': {\n",
    "        'T5-PAWS': (\n",
    "            \"During our final discussion, I told him about the new submission — the one we had been waiting for since last autumn, \"\n",
    "            \"but the updates were confusing as they did not include the full feedback from reviewer or editor. Anyway, the team, \"\n",
    "            \"although recently a bit delayed and less communicative, really tried their best for paper and cooperation. We should be \"\n",
    "            \"grateful for the acceptance and efforts until the Springer link finally came last week. Please also remind me if the doctor \"\n",
    "            \"plans to edit the acknowledgments section before sending again. I apologize if I missed that part; I didn’t see it final yet. \"\n",
    "            \"Let us make sure all are safe and celebrate the outcome with strong coffee and future targets.\"\n",
    "        ),\n",
    "        'Pegasus': (\n",
    "            \"I told him about the new submission we were waiting for, but the updates didn't include the full feedback from the reviewer \"\n",
    "            \"or editor, which was confusing. The team tried their best for paper and cooperation despite the recent delay and less communication. \"\n",
    "            \"We should be thankful for the acceptance and efforts until the Springer link came last week. If the doctor still plans to edit the \"\n",
    "            \"acknowledgments section, please remind me. I apologize if I missed it; I haven't seen it final yet. Let's make sure everyone is safe \"\n",
    "            \"and celebrate the outcome with coffee and targets.\"\n",
    "        ),\n",
    "        'Humarin': (\n",
    "            \"During our last discussion, I shared with him the new submission I had been waiting for last autumn, but the changes were unclear as \"\n",
    "            \"they did not provide the complete feedback from the reviewer or editor. Despite some delays and less communication in the past few days, \"\n",
    "            \"the team made significant efforts to improve their paper and collaboration. We should be thankful for the determination that led to the \"\n",
    "            \"Springer link finally being released last week. If the doctor plans to revise the acknowledgments section before re-submitting, please remind me. \"\n",
    "            \"I regret not noticing it yet. Let us ensure everyone's safety and commemorate this outcome with strong coffee and future goals.\"\n",
    "        )\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44cf976",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, orig in originals.items():\n",
    "    print(f\"\\n=== Cosine Similarities ({label}) ===\")\n",
    "    orig_emb = embed_text(orig).reshape(1, -1)\n",
    "\n",
    "    for name, recon in reconstructions[label].items():\n",
    "        recon_emb = embed_text(recon).reshape(1, -1)\n",
    "        cos = cosine_similarity(orig_emb, recon_emb)[0][0]\n",
    "        print(f\"{name:<8}: {cos:.4f}\")\n",
    "\n",
    "    orig_kw = extract_keywords(orig)\n",
    "    peg_kw  = extract_keywords(reconstructions[label]['Pegasus'])\n",
    "    all_kw  = sorted(set(orig_kw + peg_kw))\n",
    "    embs    = [embed_text(w) for w in all_kw]\n",
    "    coords  = PCA(n_components=2).fit_transform(embs)\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    colors = ['blue' if w in orig_kw else 'red' for w in all_kw]\n",
    "    plt.scatter(coords[:,0], coords[:,1], c=colors)\n",
    "    for i, w in enumerate(all_kw):\n",
    "        plt.annotate(w, (coords[i,0], coords[i,1]), fontsize=7)\n",
    "    plt.title(f\"PCA: {label} Original vs Pegasus Keywords\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
