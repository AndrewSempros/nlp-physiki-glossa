{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import logging\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, CFG, ChartParser\n",
    "from nltk.corpus import wordnet, treebank\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57331e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cfac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphrasingAutomaton:\n",
    "    \"\"\"\n",
    "    Αυτόματο παραφράσεων με:\n",
    "      1) Manual rewrites (exact match map)\n",
    "      2) Registered NLP techniques (functions) applied σειριακά\n",
    "    \"\"\"\n",
    "    def __init__(self, manual_map=None):\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "        try:\n",
    "            nltk.data.find('corpora/treebank')\n",
    "        except LookupError:\n",
    "            nltk.download('treebank', quiet=True)\n",
    "        try:\n",
    "            nltk.data.find('corpora/wordnet')\n",
    "        except LookupError:\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "        self.manual_map = manual_map or {}\n",
    "        self.techniques = []  \n",
    "\n",
    "    def register(self, name, fn):\n",
    "        \"\"\"Καταχωρεί τεχνική παραφράσεων: fn: str->str\"\"\"\n",
    "        self.techniques.append((name, fn))\n",
    "\n",
    "    def paraphrase(self, sentence: str) -> str:\n",
    "        s = sentence.strip()\n",
    "        if s in self.manual_map:\n",
    "            return self.manual_map[s]\n",
    "        for name, fn in self.techniques:\n",
    "            try:\n",
    "                s = fn(s)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_regex(text: str) -> str:\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_manual_syntax_tree(text: str) -> str:\n",
    "    from nltk import Tree\n",
    "    tokens = text.split()\n",
    "    def build(tokens):\n",
    "        if not tokens:\n",
    "            return None\n",
    "        if len(tokens)==1:\n",
    "            return Tree(tokens[0], [])\n",
    "        mid=len(tokens)//2\n",
    "        left=build(tokens[:mid]); right=build(tokens[mid+1:])\n",
    "        return Tree(tokens[mid], [c for c in (left,right) if c])\n",
    "    tree=build(tokens)\n",
    "    return tree.pformat() if tree else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_pos_text(text: str) -> str:\n",
    "    global _stanza_nlp\n",
    "    try:\n",
    "        _stanza_nlp\n",
    "    except NameError:\n",
    "        stanza.download('en')\n",
    "        _stanza_nlp = stanza.Pipeline(lang='en', processors='tokenize,pos', verbose=False)\n",
    "    doc=_stanza_nlp(text)\n",
    "    return ' '.join(f\"{w.text}/{w.upos}\" for sent in doc.sentences for w in sent.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab66a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fsa_token_filter(text: str) -> str:\n",
    "    def valid(word):\n",
    "        state=0\n",
    "        for c in word:\n",
    "            if state==0 and c.isalpha(): state=1\n",
    "            elif state==1 and c.isalpha(): state=1\n",
    "            else: return False\n",
    "        return state==1\n",
    "    toks=text.split()\n",
    "    return ' '.join(w for w in toks if valid(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg_parse_flatten(text: str) -> str:\n",
    "    grammar=CFG.fromstring(\"\"\"\n",
    "      S -> NP VP\n",
    "      NP -> Det N | Det Adj N\n",
    "      VP -> V NP | V\n",
    "      Det -> 'the' | 'a'\n",
    "      Adj -> 'big' | 'small'\n",
    "      N -> 'cat' | 'dog' | 'festival' | 'team'\n",
    "      V -> 'celebrate' | 'believe' | 'jumps' | 'sleeps'\n",
    "    \"\"\")\n",
    "    parser=ChartParser(grammar)\n",
    "    toks=text.lower().split()\n",
    "    try:\n",
    "        tree=next(parser.parse(toks))\n",
    "        return ' '.join(tree.leaves())\n",
    "    except StopIteration:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treebank_lemmatize(text: str) -> str:\n",
    "    lemmas=[]\n",
    "    for w in text.split():\n",
    "        if w.lower() in treebank.words()[:10]: lemmas.append(w.lower())\n",
    "        else: lemmas.append(w)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ef4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    manual_map={\n",
    "        \"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives.\":\n",
    "          \"Today marks our Dragon Boat Festival in Chinese culture, celebrating safety and prosperity in our lives.\",\n",
    "        \"Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation.\":\n",
    "          \"I believe our team, despite some delays and reduced communication recently, has done its best on the paper and in our collaboration.\"\n",
    "    }\n",
    "    automaton=ParaphrasingAutomaton(manual_map)\n",
    "    automaton.register('regex_token', tokenize_regex)\n",
    "    automaton.register('syntax_tree', build_manual_syntax_tree)\n",
    "    automaton.register('stanza_pos', stanza_pos_text)\n",
    "    automaton.register('fsa_filter', fsa_token_filter)\n",
    "    automaton.register('cfg_flatten', cfg_parse_flatten)\n",
    "    automaton.register('treebank_lemm', treebank_lemmatize)\n",
    "\n",
    "    for i,s in enumerate(manual_map.keys(),1):\n",
    "        out=automaton.paraphrase(s)\n",
    "        print(f\"=== Πρόταση {i} ===\")\n",
    "        print(f\"Original:    {s}\")\n",
    "        print(f\"Improved:    {out}\\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
